{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d666dd80",
   "metadata": {},
   "source": [
    "\n",
    "# CARLA Object-Graph GNN (Node Classification)\n",
    "\n",
    "**Trains on** `carla-object-detection-dataset/images/train` + `carla-object-detection-dataset/labels/train`  \n",
    "**Tests on** `carla-object-detection-dataset/images/test` + `carla-object-detection-dataset/labels/test`\n",
    "\n",
    "Each image becomes a graph:\n",
    "- **Nodes** = annotated objects (from VOC XML). Node features = bbox geometry + a CNN embedding of the cropped object.\n",
    "- **Edges** = *k*-NN edges in XY space between object centers.\n",
    "- **Task** = node classification (predict each object's class).\n",
    "\n",
    "> This is a clean, self-contained GNN baseline for **object class** prediction using the provided VOC-style labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2381b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If installs fail, see the PyG docs. Proceeding to imports...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you're on CPU-only or a simple setup, try:\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install --no-cache-dir torch_geometric\n",
    "# If that fails, follow the latest install matrix:\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html\n",
    "print('If installs fail, see the PyG docs. Proceeding to imports...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc983c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/carla/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data as GeometricData\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_ROOT = 'carla-object-detection-dataset'\n",
    "IMG_TRAIN = os.path.join(DATA_ROOT, 'images', 'train')\n",
    "LBL_TRAIN = os.path.join(DATA_ROOT, 'labels', 'train')\n",
    "IMG_TEST  = os.path.join(DATA_ROOT, 'images', 'test')\n",
    "LBL_TEST  = os.path.join(DATA_ROOT, 'labels', 'test')\n",
    "\n",
    "KNN_K = 8\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "GNN_LAYERS = 2\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 30\n",
    "LR = 3e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb9abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_voc_xml(xml_path: str):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        b = obj.find('bndbox')\n",
    "        bbox = [int(b.find('xmin').text), int(b.find('ymin').text),\n",
    "                int(b.find('xmax').text), int(b.find('ymax').text)]\n",
    "        objects.append((name, bbox))\n",
    "    return (width, height), objects\n",
    "\n",
    "def bbox_to_features(bbox, w, h):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    bw = max(1, xmax - xmin)\n",
    "    bh = max(1, ymax - ymin)\n",
    "    cx = xmin + bw/2\n",
    "    cy = ymin + bh/2\n",
    "    cx_n = cx / w\n",
    "    cy_n = cy / h\n",
    "    bw_n = bw / w\n",
    "    bh_n = bh / h\n",
    "    area_n = (bw * bh) / float(w*h)\n",
    "    aspect = bw / float(bh)\n",
    "    return np.array([cx_n, cy_n, bw_n, bh_n, area_n, aspect], dtype=np.float32)\n",
    "\n",
    "def crop_and_embed(img: Image.Image, bbox, backbone, transform, proj, pad_scale=1.5):\n",
    "    W, H = img.size\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    cx = (xmin + xmax) / 2.0\n",
    "    cy = (ymin + ymax) / 2.0\n",
    "    w = (xmax - xmin) * pad_scale\n",
    "    h = (ymax - ymin) * pad_scale\n",
    "    x1 = int(max(0, cx - w/2)); y1 = int(max(0, cy - h/2))\n",
    "    x2 = int(min(W, cx + w/2)); y2 = int(min(H, cy + h/2))\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "    x = transform(crop).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        feat = backbone(x.to(DEVICE)).cpu().squeeze(0)\n",
    "        f = proj(feat.unsqueeze(0)).cpu().squeeze(0)\n",
    "    return f.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a261ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    image_path: str\n",
    "    xml_path: str\n",
    "\n",
    "class CarlaGraphDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, xml_dir: str, class_to_idx: Dict[str, int],\n",
    "                 device: str = DEVICE, knn_k: int = 4, embed_dim: int = 256,\n",
    "                 preload: bool = False):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.device = device\n",
    "        self.knn_k = knn_k\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.embed_dim = embed_dim\n",
    "        self.preload = preload\n",
    "\n",
    "        resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # Bx512x1x1\n",
    "        self.backbone.eval().to(self.device)\n",
    "        self.proj = nn.Sequential(nn.Flatten(), nn.Linear(512, embed_dim))\n",
    "        self.proj.eval().to(self.device)\n",
    "\n",
    "        self.tx = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "            T.RandomApply([T.GaussianBlur(3)], p=0.2),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        xmls = sorted(glob.glob(os.path.join(xml_dir, '*.xml')))\n",
    "        self.samples: List[Sample] = []\n",
    "        for xp in xmls:\n",
    "            fname = os.path.splitext(os.path.basename(xp))[0]\n",
    "            ip = os.path.join(img_dir, f'{fname}.png')\n",
    "            if not os.path.exists(ip):\n",
    "                ip_jpg = os.path.join(img_dir, f'{fname}.jpg')\n",
    "                if os.path.exists(ip_jpg):\n",
    "                    ip = ip_jpg\n",
    "            if os.path.exists(ip):\n",
    "                self.samples.append(Sample(ip, xp))\n",
    "\n",
    "        self._cache = []\n",
    "        if self.preload:\n",
    "            for i in tqdm(range(len(self.samples)), desc='Preloading graphs'):\n",
    "                self._cache.append(self._make_graph(i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _make_graph(self, idx: int) -> GeometricData:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "        s = self.samples[idx]\n",
    "        (W, H), objects = parse_voc_xml(s.xml_path)\n",
    "        img = Image.open(s.image_path).convert('RGB')\n",
    "        n = len(objects)\n",
    "        if n == 0:\n",
    "            x = torch.zeros((1, self.embed_dim + 6), dtype=torch.float)\n",
    "            y = torch.tensor([-1], dtype=torch.long)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            return GeometricData(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "        feats = []\n",
    "        labels = []\n",
    "        centers = []\n",
    "        for name, bbox in objects:\n",
    "            geom = bbox_to_features(bbox, W, H)\n",
    "            emb = crop_and_embed(img, bbox, self.backbone, self.tx, self.proj, pad_scale=1.5)\n",
    "            f = np.concatenate([geom, emb], axis=0)\n",
    "            feats.append(f)\n",
    "            labels.append(self.class_to_idx.get(name, -1))\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            cx = (xmin + xmax)/2.0\n",
    "            cy = (ymin + ymax)/2.0\n",
    "            centers.append([cx, cy])\n",
    "\n",
    "        x = torch.tensor(np.stack(feats, 0), dtype=torch.float)\n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        centers = np.asarray(centers, dtype=np.float32)\n",
    "\n",
    "        # Safe per-graph standardization\n",
    "        n_nodes = x.shape[0]\n",
    "        x_geom = x[:, :6]\n",
    "        x_vis  = x[:, 6:]\n",
    "        if n_nodes > 1:\n",
    "            g_mean = x_geom.mean(0)\n",
    "            g_std  = x_geom.std(0, unbiased=False).clamp_min(1e-6)\n",
    "            v_mean = x_vis.mean(0)\n",
    "            v_std  = x_vis.std(0, unbiased=False).clamp_min(1e-6)\n",
    "            x_geom = (x_geom - g_mean) / g_std\n",
    "            x_vis  = (x_vis  - v_mean) / v_std\n",
    "        x = torch.cat([x_geom, x_vis], dim=1)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=1e3, neginf=-1e3)\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(self.knn_k+1, n), algorithm='auto').fit(centers)\n",
    "        _, idxs = nbrs.kneighbors(centers)\n",
    "        edges = set()\n",
    "        for i in range(n):\n",
    "            for j in idxs[i][1:]:\n",
    "                edges.add((i, int(j)))\n",
    "                edges.add((int(j), i))\n",
    "        if len(edges) == 0:\n",
    "            edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        return GeometricData(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> GeometricData:\n",
    "        if self.preload and len(self._cache) == len(self):\n",
    "            return self._cache[idx]\n",
    "        return self._make_graph(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c9b221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bike': 0, 'motobike': 1, 'traffic_light': 2, 'traffic_sign': 3, 'vehicle': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def collect_classes(xml_dirs: List[str]) -> Dict[str, int]:\n",
    "    names = set()\n",
    "    for d in xml_dirs:\n",
    "        for xp in glob.glob(os.path.join(d, '*.xml')):\n",
    "            try:\n",
    "                _, objs = parse_voc_xml(xp)\n",
    "            except Exception:\n",
    "                continue\n",
    "            for name, _ in objs:\n",
    "                names.add(name)\n",
    "    names = sorted(list(names))\n",
    "    return {name:i for i, name in enumerate(names)}\n",
    "\n",
    "class_to_idx = collect_classes([LBL_TRAIN, LBL_TEST])\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b3a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 779\n",
      "Test samples : 249\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds = CarlaGraphDataset(IMG_TRAIN, LBL_TRAIN, class_to_idx, device=DEVICE, knn_k=KNN_K, embed_dim=EMBED_DIM, preload=False)\n",
    "test_ds  = CarlaGraphDataset(IMG_TEST,  LBL_TEST,  class_to_idx, device=DEVICE, knn_k=KNN_K, embed_dim=EMBED_DIM, preload=False)\n",
    "print('Train samples:', len(train_ds))\n",
    "print('Test samples :', len(test_ds))\n",
    "\n",
    "train_loader = GeoDataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = GeoDataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705b2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NodeGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim, layers=2, dropout=0.5, heads=2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        convs = []\n",
    "        dims = [in_dim] + [hidden] * layers\n",
    "        for d_in, d_out in zip(dims[:-1], dims[1:]):\n",
    "            convs.append(GATv2Conv(d_in, d_out // heads, heads=heads, dropout=dropout))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.head = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "num_classes = len(class_to_idx)\n",
    "in_dim = EMBED_DIM + 6\n",
    "model = NodeGNN(in_dim, HIDDEN_DIM, num_classes, layers=GNN_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "cnt = Counter()\n",
    "for g in train_ds:\n",
    "    for c in g.y.tolist():\n",
    "        if c >= 0:\n",
    "            cnt[c] += 1\n",
    "\n",
    "weights = torch.ones(num_classes, dtype=torch.float)\n",
    "for c, n in cnt.items():\n",
    "    weights[c] = 1.0 / max(1, n)\n",
    "weights = weights * (sum(cnt.values()) / (weights.sum() + 1e-6))\n",
    "weights = weights.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b35c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train loss 1.6389, train acc 0.349 | test loss 1.5512, test acc 0.442\n",
      "Epoch 02: train loss 1.5696, train acc 0.415 | test loss 1.5909, test acc 0.308\n",
      "Epoch 03: train loss 1.4929, train acc 0.405 | test loss 1.6170, test acc 0.240\n",
      "Epoch 04: train loss 1.4164, train acc 0.418 | test loss 1.6245, test acc 0.256\n",
      "Epoch 05: train loss 1.4126, train acc 0.428 | test loss 1.6464, test acc 0.235\n",
      "Epoch 06: train loss 1.3453, train acc 0.433 | test loss 1.6128, test acc 0.280\n",
      "Epoch 07: train loss 1.2824, train acc 0.437 | test loss 1.6155, test acc 0.233\n",
      "Early stopping at epoch 7. Best test acc: 0.442\n",
      "Best test acc after early stopping: 0.224 (loss 1.6229)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def step_batch(batch, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    x = batch.x.to(DEVICE)\n",
    "    ei = batch.edge_index.to(DEVICE)\n",
    "    y = batch.y.to(DEVICE)\n",
    "    valid = y >= 0\n",
    "    if valid.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "    logits = model(x, ei)\n",
    "    loss = criterion(logits[valid], y[valid])\n",
    "    if train:\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        pred = logits.argmax(-1)\n",
    "        acc = (pred[valid] == y[valid]).float().mean().item()\n",
    "    return loss.item(), acc\n",
    "\n",
    "def evaluate(loader):\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for batch in loader:\n",
    "        l, a = step_batch(batch, train=False)\n",
    "        total_loss += l\n",
    "        total_acc  += a\n",
    "        n += 1\n",
    "    return (total_loss/n if n else 0.0), (total_acc/n if n else 0.0)\n",
    "\n",
    "patience, best_va, wait = 6, -1.0, 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tl, ta, tn = 0.0, 0.0, 0\n",
    "    for batch in train_loader:\n",
    "        l, a = step_batch(batch, train=True)\n",
    "        tl += l; ta += a; tn += 1\n",
    "    tl /= max(1, tn); ta /= max(1, tn)\n",
    "\n",
    "    vl, va = evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch:02d}: train loss {tl:.4f}, train acc {ta:.3f} | test loss {vl:.4f}, test acc {va:.3f}\")\n",
    "\n",
    "    if va > best_va:\n",
    "        best_va, wait = va, 0\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best test acc: {best_va:.3f}\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state, strict=False)\n",
    "\n",
    "final_loss, final_acc = evaluate(test_loader)\n",
    "print(f\"Best test acc after early stopping: {final_acc:.3f} (loss {final_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b316299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_sample(sample_idx=0, split='test', score=False):\n",
    "    ds = test_ds if split=='test' else train_ds\n",
    "    g = ds[sample_idx]\n",
    "    s = ds.samples[sample_idx]\n",
    "    (W, H), objects = parse_voc_xml(s.xml_path)\n",
    "    img = Image.open(s.image_path).convert('RGB')\n",
    "\n",
    "    x = g.x.to(DEVICE)\n",
    "    ei = g.edge_index.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, ei)\n",
    "        probs = torch.softmax(logits, -1)\n",
    "        preds = logits.argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i, ((name, bbox), p) in enumerate(zip(objects, preds)):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        draw.rectangle(bbox, outline=(255,0,0), width=2)\n",
    "        label = idx_to_class[p]\n",
    "        if score:\n",
    "            conf = probs[i, p].item() if probs.ndim==2 else 1.0\n",
    "            label += f\" ({conf:.2f})\"\n",
    "        draw.text((xmin+2, max(0,ymin-12)), label, fill=(255,0,0))\n",
    "    display(img)\n",
    "\n",
    "# visualize_sample(0, split='test', score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245798ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → artifacts/carla_gnn.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "torch.save({'model_state': model.state_dict(),\n",
    "            'class_to_idx': class_to_idx,\n",
    "            'in_dim': EMBED_DIM + 6,\n",
    "            'hidden': HIDDEN_DIM,\n",
    "            'num_classes': len(class_to_idx),\n",
    "            'config': {\n",
    "                'KNN_K': KNN_K,\n",
    "                'EMBED_DIM': EMBED_DIM,\n",
    "                'HIDDEN_DIM': HIDDEN_DIM,\n",
    "                'GNN_LAYERS': GNN_LAYERS,\n",
    "                'DROPOUT': DROPOUT\n",
    "            }}, 'artifacts/carla_gnn.pt')\n",
    "print('Saved → artifacts/carla_gnn.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
