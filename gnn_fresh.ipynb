{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a5fb75",
   "metadata": {},
   "source": [
    "# CARLA Object-Graph GNN (Node Classification)\n",
    "\n",
    "This notebook trains a **graph neural network (GNN)** on the CARLA object-detection\n",
    "dataset, treating each image as a graph:\n",
    "\n",
    "- **Nodes** = annotated objects (from VOC-style XML).\n",
    "  - Node features = bbox geometry + CNN embedding of the cropped object.\n",
    "- **Edges** = *k*-NN edges in XY space between object centers.\n",
    "- **Task** = node classification (predict each object's class label).\n",
    "\n",
    "The pipeline:\n",
    "\n",
    "1. Build `class_to_idx` from all labels.\n",
    "2. For each image, build a graph:\n",
    "   - Extract object crops.\n",
    "   - Embed with ResNet-18 (ImageNet-pretrained).\n",
    "   - Concatenate geometry features.\n",
    "   - Build k-NN edges.\n",
    "3. Train a GATv2-based node classifier with\n",
    "   - class-balanced weights,\n",
    "   - validation-based early stopping.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9ce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/carla/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch_geometric.data import Data as GeometricData\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_ROOT = 'carla-object-detection-dataset'\n",
    "IMG_TRAIN = os.path.join(DATA_ROOT, 'images', 'train')\n",
    "LBL_TRAIN = os.path.join(DATA_ROOT, 'labels', 'train')\n",
    "IMG_TEST  = os.path.join(DATA_ROOT, 'images', 'test')\n",
    "LBL_TEST  = os.path.join(DATA_ROOT, 'labels', 'test')\n",
    "\n",
    "KNN_K      = 8\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 256\n",
    "GNN_LAYERS = 2\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT    = 0.5\n",
    "EPOCHS     = 40\n",
    "LR         = 3e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "PATIENCE   = 8\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da8c28",
   "metadata": {},
   "source": [
    "## 1) VOC Parsing & Geometry Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb59bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_xml(xml_path: str) -> Tuple[Tuple[int, int], List[Tuple[str, List[int]]]]:\n",
    "    \"\"\"\n",
    "    Parse VOC-style XML and return:\n",
    "    - (width, height)\n",
    "    - list of (class_name, [xmin, ymin, xmax, ymax])\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        b = obj.find('bndbox')\n",
    "        bbox = [\n",
    "            int(b.find('xmin').text),\n",
    "            int(b.find('ymin').text),\n",
    "            int(b.find('xmax').text),\n",
    "            int(b.find('ymax').text),\n",
    "        ]\n",
    "        objects.append((name, bbox))\n",
    "\n",
    "    return (width, height), objects\n",
    "\n",
    "\n",
    "def bbox_to_features(bbox, w: int, h: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a bounding box into normalized geometry features:\n",
    "      [cx_norm, cy_norm, bw_norm, bh_norm, area_norm, aspect_ratio]\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    bw = max(1, xmax - xmin)\n",
    "    bh = max(1, ymax - ymin)\n",
    "    cx = xmin + bw / 2.0\n",
    "    cy = ymin + bh / 2.0\n",
    "\n",
    "    cx_n = cx / w\n",
    "    cy_n = cy / h\n",
    "    bw_n = bw / w\n",
    "    bh_n = bh / h\n",
    "    area_n = (bw * bh) / float(w * h)\n",
    "    aspect = bw / float(bh)\n",
    "\n",
    "    return np.array([cx_n, cy_n, bw_n, bh_n, area_n, aspect], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2d2ce",
   "metadata": {},
   "source": [
    "## 2) CNN Backbone for Visual Embeddings\n",
    "\n",
    "We use a pretrained ResNet-18 as a crop encoder and project to `EMBED_DIM`.\n",
    "\n",
    "We will:\n",
    "- Use **stronger transforms** (jitter, blur) for **training crops**.\n",
    "- Use **deterministic** transforms (resize + normalize) for **val/test** crops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(\n",
    "    weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    ")\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "backbone.eval().to(DEVICE)\n",
    "\n",
    "proj = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, EMBED_DIM),\n",
    ")\n",
    "proj.eval().to(DEVICE)\n",
    "\n",
    "\n",
    "train_tx = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    T.RandomApply([T.GaussianBlur(3)], p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "eval_tx = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def crop_and_embed(\n",
    "    img: Image.Image,\n",
    "    bbox,\n",
    "    backbone: nn.Module,\n",
    "    transform,\n",
    "    proj: nn.Module,\n",
    "    pad_scale: float = 1.5,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crop padded region around bbox, embed with CNN backbone and projection.\n",
    "    \"\"\"\n",
    "    W, H = img.size\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    cx = (xmin + xmax) / 2.0\n",
    "    cy = (ymin + ymax) / 2.0\n",
    "    w = (xmax - xmin) * pad_scale\n",
    "    h = (ymax - ymin) * pad_scale\n",
    "\n",
    "    x1 = int(max(0, cx - w / 2.0))\n",
    "    y1 = int(max(0, cy - h / 2.0))\n",
    "    x2 = int(min(W, cx + w / 2.0))\n",
    "    y2 = int(min(H, cy + h / 2.0))\n",
    "\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "    x = transform(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feat = backbone(x)\n",
    "        emb  = proj(feat).cpu()\n",
    "\n",
    "    return emb.squeeze(0).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb6f2f",
   "metadata": {},
   "source": [
    "## 3) Dataset & Graph Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955dd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    image_path: str\n",
    "    xml_path: str\n",
    "\n",
    "\n",
    "class CarlaGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of graphs, one graph per image.\n",
    "\n",
    "    For each XML:\n",
    "      - Parse objects and bboxes.\n",
    "      - Extract geometry + visual features per object.\n",
    "      - Build k-NN graph in (cx, cy) space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        xml_dir: str,\n",
    "        class_to_idx: Dict[str, int],\n",
    "        backbone: nn.Module,\n",
    "        proj: nn.Module,\n",
    "        transform,\n",
    "        device: str = DEVICE,\n",
    "        knn_k: int = 4,\n",
    "        embed_dim: int = 256,\n",
    "        preload: bool = False,\n",
    "        name: str = \"split\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.device = device\n",
    "        self.knn_k = knn_k\n",
    "        self.embed_dim = embed_dim\n",
    "        self.preload = preload\n",
    "        self.name = name\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.proj = proj\n",
    "        self.transform = transform\n",
    "\n",
    "        xmls = sorted(glob.glob(os.path.join(xml_dir, \"*.xml\")))\n",
    "        self.samples: List[Sample] = []\n",
    "        for xp in xmls:\n",
    "            base = os.path.splitext(os.path.basename(xp))[0]\n",
    "            ip_png = os.path.join(img_dir, f\"{base}.png\")\n",
    "            ip_jpg = os.path.join(img_dir, f\"{base}.jpg\")\n",
    "\n",
    "            if os.path.exists(ip_png):\n",
    "                ip = ip_png\n",
    "            elif os.path.exists(ip_jpg):\n",
    "                ip = ip_jpg\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            self.samples.append(Sample(ip, xp))\n",
    "\n",
    "        self._cache = []\n",
    "        if self.preload:\n",
    "            print(f\"[{self.name}] Preloading {len(self.samples)} graphs...\")\n",
    "            for i in tqdm(range(len(self.samples)), desc=f\"Preloading-{self.name}\"):\n",
    "                self._cache.append(self._make_graph(i))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _make_graph(self, idx: int) -> GeometricData:\n",
    "        s = self.samples[idx]\n",
    "        (W, H), objects = parse_voc_xml(s.xml_path)\n",
    "        img = Image.open(s.image_path).convert(\"RGB\")\n",
    "\n",
    "        n = len(objects)\n",
    "        if n == 0:\n",
    "            x = torch.zeros((1, self.embed_dim + 6), dtype=torch.float)\n",
    "            y = torch.tensor([-1], dtype=torch.long)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            return GeometricData(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "        feats = []\n",
    "        labels = []\n",
    "        centers = []\n",
    "\n",
    "        for name, bbox in objects:\n",
    "            geom = bbox_to_features(bbox, W, H)\n",
    "            emb  = crop_and_embed(img, bbox, self.backbone, self.transform, self.proj)\n",
    "            f = np.concatenate([geom, emb], axis=0)\n",
    "            feats.append(f)\n",
    "            labels.append(self.class_to_idx.get(name, -1))\n",
    "\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            cx = (xmin + xmax) / 2.0\n",
    "            cy = (ymin + ymax) / 2.0\n",
    "            centers.append([cx, cy])\n",
    "\n",
    "        x = torch.tensor(np.stack(feats, axis=0), dtype=torch.float)\n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        centers = np.asarray(centers, dtype=np.float32)\n",
    "\n",
    "        x_geom = x[:, :6]\n",
    "        x_vis  = x[:, 6:]\n",
    "        if x.shape[0] > 1:\n",
    "            g_mean = x_geom.mean(0)\n",
    "            g_std  = x_geom.std(0, unbiased=False).clamp_min(1e-6)\n",
    "            v_mean = x_vis.mean(0)\n",
    "            v_std  = x_vis.std(0, unbiased=False).clamp_min(1e-6)\n",
    "\n",
    "            x_geom = (x_geom - g_mean) / g_std\n",
    "            x_vis  = (x_vis  - v_mean) / v_std\n",
    "\n",
    "        x = torch.cat([x_geom, x_vis], dim=1)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=1e3, neginf=-1e3)\n",
    "\n",
    "        n_nodes = x.shape[0]\n",
    "        knn_k = min(self.knn_k + 1, n_nodes)\n",
    "        nbrs = NearestNeighbors(n_neighbors=knn_k, algorithm=\"auto\").fit(centers)\n",
    "        _, idxs = nbrs.kneighbors(centers)\n",
    "\n",
    "        edges = set()\n",
    "        for i in range(n_nodes):\n",
    "            for j in idxs[i][1:]:\n",
    "                j = int(j)\n",
    "                edges.add((i, j))\n",
    "                edges.add((j, i))\n",
    "\n",
    "        if not edges:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "        return GeometricData(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> GeometricData:\n",
    "        if self.preload and len(self._cache) == len(self.samples):\n",
    "            return self._cache[idx]\n",
    "        return self._make_graph(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba71ba0",
   "metadata": {},
   "source": [
    "## 4) Class Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6377644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n",
      "class_to_idx: {'bike': 0, 'motobike': 1, 'traffic_light': 2, 'traffic_sign': 3, 'vehicle': 4}\n"
     ]
    }
   ],
   "source": [
    "def collect_classes(xml_dirs: List[str]) -> Dict[str, int]:\n",
    "    names = set()\n",
    "    for d in xml_dirs:\n",
    "        for xp in glob.glob(os.path.join(d, \"*.xml\")):\n",
    "            try:\n",
    "                _, objs = parse_voc_xml(xp)\n",
    "            except Exception:\n",
    "                continue\n",
    "            for name, _ in objs:\n",
    "                names.add(name)\n",
    "    names = sorted(names)\n",
    "    return {name: i for i, name in enumerate(names)}\n",
    "\n",
    "\n",
    "class_to_idx = collect_classes([LBL_TRAIN, LBL_TEST])\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "print(\"Number of classes:\", len(class_to_idx))\n",
    "print(\"class_to_idx:\", class_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6281b",
   "metadata": {},
   "source": [
    "## 5) Dataset Instances & Split (Train / Val / Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9680fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train+val images: 779\n",
      "Total test images     : 249\n",
      "Train images: 624, Val images: 155\n"
     ]
    }
   ],
   "source": [
    "full_train_ds = CarlaGraphDataset(\n",
    "    IMG_TRAIN, LBL_TRAIN,\n",
    "    class_to_idx=class_to_idx,\n",
    "    backbone=backbone,\n",
    "    proj=proj,\n",
    "    transform=train_tx,\n",
    "    device=DEVICE,\n",
    "    knn_k=KNN_K,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    preload=False,\n",
    "    name=\"train+val\",\n",
    ")\n",
    "\n",
    "test_ds = CarlaGraphDataset(\n",
    "    IMG_TEST, LBL_TEST,\n",
    "    class_to_idx=class_to_idx,\n",
    "    backbone=backbone,\n",
    "    proj=proj,\n",
    "    transform=eval_tx,\n",
    "    device=DEVICE,\n",
    "    knn_k=KNN_K,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    preload=False,\n",
    "    name=\"test\",\n",
    ")\n",
    "\n",
    "print(\"Total train+val images:\", len(full_train_ds))\n",
    "print(\"Total test images     :\", len(test_ds))\n",
    "\n",
    "indices = list(range(len(full_train_ds)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "val_size = int(len(indices) * VAL_FRACTION)\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "train_ds = Subset(full_train_ds, train_indices)\n",
    "val_ds   = Subset(full_train_ds, val_indices)\n",
    "\n",
    "print(f\"Train images: {len(train_ds)}, Val images: {len(val_ds)}\")\n",
    "\n",
    "train_loader = GeoDataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = GeoDataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = GeoDataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9da8bf",
   "metadata": {},
   "source": [
    "### 5.1) Class Distribution (for train subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting train labels: 100%|██████████| 624/624 [00:33<00:00, 18.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts (node-level):\n",
      "  bike           : 121\n",
      "  motobike       : 63\n",
      "  traffic_light  : 802\n",
      "  traffic_sign   : 77\n",
      "  vehicle        : 932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_label_counts = Counter()\n",
    "for g in tqdm(train_ds, desc=\"Counting train labels\"):\n",
    "    y = g.y.numpy()\n",
    "    for c in y:\n",
    "        if c >= 0:\n",
    "            train_label_counts[int(c)] += 1\n",
    "\n",
    "print(\"Train label counts (node-level):\")\n",
    "for idx, cnt in sorted(train_label_counts.items()):\n",
    "    print(f\"  {idx_to_class[idx]:15s}: {cnt}\")\n",
    "\n",
    "num_classes = len(class_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c44ebe",
   "metadata": {},
   "source": [
    "## 6) GNN Model\n",
    "\n",
    "We use a simple stack of GATv2Conv layers with ReLU + dropout,\n",
    "followed by a linear classification head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12440edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeGNN(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATv2Conv(262, 128, heads=2)\n",
       "    (1): GATv2Conv(256, 128, heads=2)\n",
       "  )\n",
       "  (head): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NodeGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden: int,\n",
    "        out_dim: int,\n",
    "        layers: int = 2,\n",
    "        dropout: float = 0.5,\n",
    "        heads: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        convs = []\n",
    "        dims = [in_dim] + [hidden] * layers\n",
    "        for d_in, d_out in zip(dims[:-1], dims[1:]):\n",
    "            convs.append(\n",
    "                GATv2Conv(\n",
    "                    in_channels=d_in,\n",
    "                    out_channels=d_out // heads,\n",
    "                    heads=heads,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.head = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "in_dim = EMBED_DIM + 6\n",
    "model = NodeGNN(\n",
    "    in_dim=in_dim,\n",
    "    hidden=HIDDEN_DIM,\n",
    "    out_dim=num_classes,\n",
    "    layers=GNN_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    heads=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e50cb",
   "metadata": {},
   "source": [
    "## 7) Loss Function, Class Weights, Optimizer & Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a00148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [121.0, 63.0, 802.0, 77.0, 932.0]\n",
      "Class weights: [1.047610878944397, 2.012078285217285, 0.15805602073669434, 1.6462457180023193, 0.1360095739364624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/carla/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class_counts = torch.zeros(num_classes, dtype=torch.float)\n",
    "\n",
    "for g in train_ds:\n",
    "    y = g.y\n",
    "    valid = y[y >= 0]\n",
    "    for c in valid.tolist():\n",
    "        class_counts[c] += 1\n",
    "\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.mean() \n",
    "class_weights = class_weights.to(DEVICE)\n",
    "\n",
    "print(\"Class counts:\", class_counts.tolist())\n",
    "print(\"Class weights:\", class_weights.tolist())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e3a13",
   "metadata": {},
   "source": [
    "## 8) Training & Evaluation Loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaff5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_batch(batch, train: bool = True):\n",
    "    \"\"\"\n",
    "    One forward/backward pass on a batch.\n",
    "    Returns: (loss, accuracy).\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    x = batch.x.to(DEVICE)\n",
    "    ei = batch.edge_index.to(DEVICE)\n",
    "    y = batch.y.to(DEVICE)\n",
    "\n",
    "    valid_mask = y >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    logits = model(x, ei)\n",
    "\n",
    "    loss = criterion(logits[valid_mask], y[valid_mask])\n",
    "\n",
    "    if train:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds[valid_mask] == y[valid_mask]).float().mean().item()\n",
    "\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        l, a = step_batch(batch, train=False)\n",
    "        total_loss += l\n",
    "        total_acc += a\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        return 0.0, 0.0\n",
    "    return total_loss / n_batches, total_acc / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a0e7a",
   "metadata": {},
   "source": [
    "## 9) Main Training Loop (with Validation Early Stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300efcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train loss 1.6958, train acc 0.349 | val loss 1.5497, val acc 0.500\n",
      "Epoch 02: train loss 1.6103, train acc 0.396 | val loss 1.5103, val acc 0.503\n",
      "Epoch 03: train loss 1.4971, train acc 0.407 | val loss 1.4384, val acc 0.473\n",
      "Epoch 04: train loss 1.4715, train acc 0.445 | val loss 1.3866, val acc 0.530\n",
      "Epoch 05: train loss 1.4325, train acc 0.408 | val loss 1.3504, val acc 0.467\n",
      "Epoch 06: train loss 1.4002, train acc 0.420 | val loss 1.3049, val acc 0.558\n",
      "Epoch 07: train loss 1.3505, train acc 0.434 | val loss 1.2434, val acc 0.471\n",
      "Epoch 08: train loss 1.2822, train acc 0.418 | val loss 1.2115, val acc 0.527\n",
      "Epoch 09: train loss 1.2373, train acc 0.429 | val loss 1.1635, val acc 0.519\n",
      "Epoch 10: train loss 1.1891, train acc 0.460 | val loss 1.1578, val acc 0.545\n",
      "Epoch 11: train loss 1.1955, train acc 0.462 | val loss 1.1246, val acc 0.497\n",
      "Epoch 12: train loss 1.2085, train acc 0.456 | val loss 1.1097, val acc 0.511\n",
      "Epoch 13: train loss 1.1006, train acc 0.473 | val loss 1.1314, val acc 0.522\n",
      "Epoch 14: train loss 1.1562, train acc 0.453 | val loss 1.0626, val acc 0.517\n",
      "Early stopping at epoch 14. Best val acc: 0.558 at epoch 6\n",
      "Best val acc: 0.558 at epoch 6\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = -1.0\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "wait = 0  \n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss_sum, train_acc_sum, train_batches = 0.0, 0.0, 0\n",
    "    for batch in train_loader:\n",
    "        l, a = step_batch(batch, train=True)\n",
    "        train_loss_sum += l\n",
    "        train_acc_sum += a\n",
    "        train_batches += 1\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_batches)\n",
    "    train_acc = train_acc_sum / max(1, train_batches)\n",
    "\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}: \"\n",
    "        f\"train loss {train_loss:.4f}, train acc {train_acc:.3f} | \"\n",
    "        f\"val loss {val_loss:.4f}, val acc {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        wait = 0\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\n",
    "                f\"Early stopping at epoch {epoch}. \"\n",
    "                f\"Best val acc: {best_val_acc:.3f} at epoch {best_epoch}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state, strict=False)\n",
    "\n",
    "print(f\"Best val acc: {best_val_acc:.3f} at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab71e46",
   "metadata": {},
   "source": [
    "## 10) Final Test Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03521461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss: 0.9726, test acc: 0.721\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(test_loader)\n",
    "print(f\"Final test loss: {test_loss:.4f}, test acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb4c27",
   "metadata": {},
   "source": [
    "### 10.1) Per-Class Test Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8ef102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bike      0.100     0.233     0.140        30\n",
      "     motobike      0.102     0.600     0.174        10\n",
      "traffic_light      0.891     0.843     0.866      1586\n",
      " traffic_sign      0.179     0.700     0.286        10\n",
      "      vehicle      0.222     0.187     0.203       203\n",
      "\n",
      "     accuracy                          0.759      1839\n",
      "    macro avg      0.299     0.513     0.334      1839\n",
      " weighted avg      0.796     0.759     0.775      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    x = batch.x.to(DEVICE)\n",
    "    ei = batch.edge_index.to(DEVICE)\n",
    "    y = batch.y.to(DEVICE)\n",
    "\n",
    "    valid_mask = y >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, ei)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "    all_targets.extend(y[valid_mask].cpu().numpy().tolist())\n",
    "    all_preds.extend(preds[valid_mask].cpu().numpy().tolist())\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        all_targets,\n",
    "        all_preds,\n",
    "        target_names=[idx_to_class[i] for i in range(num_classes)],\n",
    "        digits=3,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0feb7e",
   "metadata": {},
   "source": [
    "## 11) Visualizing Predictions on a Sample Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(sample_idx=0, split='test', score: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize predicted labels overlaid on an image.\n",
    "    \"\"\"\n",
    "    if split == 'test':\n",
    "        ds = test_ds\n",
    "    elif split == 'train':\n",
    "        ds = full_train_ds\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train' or 'test'\")\n",
    "\n",
    "    g = ds[sample_idx]\n",
    "    s = ds.samples[sample_idx]\n",
    "    (W, H), objects = parse_voc_xml(s.xml_path)\n",
    "    img = Image.open(s.image_path).convert('RGB')\n",
    "\n",
    "    x = g.x.to(DEVICE)\n",
    "    ei = g.edge_index.to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, ei)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy().tolist()\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i, ((name, bbox), p) in enumerate(zip(objects, preds)):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        draw.rectangle(bbox, outline=(255, 0, 0), width=2)\n",
    "\n",
    "        label = idx_to_class[p]\n",
    "        if score:\n",
    "            conf = probs[i, p].item()\n",
    "            label += f\" ({conf:.2f})\"\n",
    "\n",
    "        text_pos = (xmin + 2, max(0, ymin - 12))\n",
    "        draw.text(text_pos, label, fill=(255, 0, 0))\n",
    "\n",
    "    display(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f80849",
   "metadata": {},
   "source": [
    "## 12) Save Trained Model & Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca85feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → artifacts/carla_gnn_node_cls.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"class_to_idx\": class_to_idx,\n",
    "        \"idx_to_class\": idx_to_class,\n",
    "        \"in_dim\": in_dim,\n",
    "        \"hidden\": HIDDEN_DIM,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"config\": {\n",
    "            \"KNN_K\": KNN_K,\n",
    "            \"EMBED_DIM\": EMBED_DIM,\n",
    "            \"HIDDEN_DIM\": HIDDEN_DIM,\n",
    "            \"GNN_LAYERS\": GNN_LAYERS,\n",
    "            \"DROPOUT\": DROPOUT,\n",
    "            \"LR\": LR,\n",
    "            \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "            \"PATIENCE\": PATIENCE,\n",
    "            \"VAL_FRACTION\": VAL_FRACTION,\n",
    "        },\n",
    "        \"device\": str(DEVICE),\n",
    "    },\n",
    "    \"artifacts/carla_gnn_node_cls.pt\",\n",
    ")\n",
    "\n",
    "print(\"Saved → artifacts/carla_gnn_node_cls.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
